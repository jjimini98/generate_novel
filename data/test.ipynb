{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "# !pip install torch torchvision torchaudio"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "# import random \n",
    "# import json \n",
    "# import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# with open('training_corpus.json', 'r', encoding='utf-8') as f : \n",
    "#     corpus = json.load(f) #653개 \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')\n",
    "# tokenizer.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tokenizer.__dict__"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# randomList = random.sample(corpus, 10) #랜덤으로 10개 샘플링 \n",
    "# model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# for random_novel in randomList:\n",
    "#     cor = random_novel['corpus']\n",
    "#     for sentence in cor : \n",
    "#         input_ids = tokenizer.tokenize(sentence)\n",
    "\n",
    "#         # gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "#         #                    max_length=128,\n",
    "#         #                    repetition_penalty=2.0,\n",
    "#         #                    pad_token_id=tokenizer.pad_token_id,\n",
    "#         #                    eos_token_id=tokenizer.eos_token_id,\n",
    "#         #                    bos_token_id=tokenizer.bos_token_id,\n",
    "#         #                    use_cache=True)\n",
    "\n",
    "#         # generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "\n",
    "#         print(input_ids)\n",
    "#         # print(generated)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 무작위로 소설 가지고 오기"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "# from leevi_common.database.mongodb import MongoDB\n",
    "# import random \n",
    "# import json \n",
    "# import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# mongo에 저장되어있는 데이터 전체를 가지고 오기 \n",
    "\n",
    "# mongo = MongoDB(host=\"office.leevi.co.kr\", port=35005, id =\"leevi\", password = \"qlenfrl999\", database=\"whowant\" )\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')\n",
    "\n",
    "# training_corpus_list = list() #총 0.8초 소요 / 653개 \n",
    "\n",
    "# all_data = mongo.find(\"test\",{})\n",
    "\n",
    "# for x in all_data:\n",
    "#     training_corpus = dict() \n",
    "#     training_corpus['title'] = x.get('title') \n",
    "#     training_corpus['corpus'] =  x.get('corpus')\n",
    "#     training_corpus_list.append(training_corpus)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# randomList = random.sample(training_corpus_list,  10)#랜덤으로 10개 샘플링\n",
    "# randomList = random.sample(corpus, 5)#랜덤으로 1개 샘플링"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# result = list() \n",
    "# model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# for random_novel in randomList:\n",
    "#     random_result = dict()\n",
    "#     sumUNK = 0 \n",
    "#     sumToken = list()\n",
    "\n",
    "#     random_result['title'] = random_novel.get('title') #corpus 구분을 위해 소설제목 추가 \n",
    "#     cor = random_novel['corpus'] \n",
    "\n",
    "#     for sentence in cor : #소설 문장 한 줄씩 \n",
    "#         input_ids = tokenizer.tokenize(sentence)\n",
    "#         if input_ids.count('<unk>') != 0 : \n",
    "#             random_result[f'corpus_sentence : {cor.index(sentence)}'] = sentence\n",
    "#             random_result[f'tokenize_result : {cor.index(sentence)}'] = input_ids\n",
    "\n",
    "#         sumUNK += input_ids.count('<unk>')\n",
    "\n",
    "#     random_result['sumUNK'] = sumUNK\n",
    "#     result.append(random_result)\n",
    "# print(result)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 전체 소설 개수 중 UNK의 개수"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# sumUNK = 0\n",
    "# for novel in cor:\n",
    "#     cor = novel['corpus'] \n",
    "\n",
    "#     for sentence in cor : #소설 문장 한 줄씩 \n",
    "#         input_ids = tokenizer.tokenize(sentence)\n",
    "#         sumUNK += input_ids.count('<unk>')\n",
    "\n",
    "# print(sumUNK) #전체 653개에서 등장한 UNK 개수는 1845개 \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------------------------------------------------\r\n",
    "## 0813 UNK의 개수 "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !pip install pymongo\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jimin/workspace/leevi-python-base')\n",
    "# print(sys.path)\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import sentencepiece as spm\n",
    "from leevi_common.database.mongodb import MongoDB\n",
    "import random \n",
    "import json \n",
    "import torch\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# mongo에 저장되어있는 데이터 전체를 가지고 오기 \n",
    "\n",
    "mongo = MongoDB(host=\"office.leevi.co.kr\", port=35005, id =\"leevi\", password = \"qlenfrl999\", database=\"whowant\" )\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')\n",
    "\n",
    "training_corpus_list = list() #총 0.8초 소요 / 653개 \n",
    "\n",
    "all_data = mongo.find(\"test\",{})\n",
    "\n",
    "for x in all_data:\n",
    "    training_corpus = dict() \n",
    "    training_corpus['title'] = x.get('title') \n",
    "    training_corpus['corpus'] =  x.get('corpus')\n",
    "    training_corpus_list.append(training_corpus)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# training_corpus_list = [{title:~~, corpus:~~},{title:~~, corpus:~~},...]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 전체 데이터 중 unk의 개수 \n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# sumUNK = 0\n",
    "# for novel in training_corpus_list:\n",
    "#     cor = novel['corpus'] \n",
    "\n",
    "#     for sentence in cor : #소설 문장 한 줄씩 \n",
    "#         input_ids = tokenizer.tokenize(sentence)\n",
    "#         sumUNK += input_ids.count('<unk>')\n",
    "\n",
    "# print(sumUNK) #전체 653개에서 등장한 UNK 개수는 1845개 "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# randomList = random.sample(training_corpus_list,  10)#랜덤으로 10개 샘플링"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# {title : 소설명 , sumUNK : unk 개수}\n",
    "\n",
    "# result = list() \n",
    "# # model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# for random_novel in randomList:\n",
    "#     random_result = dict()\n",
    "#     sumUNK = 0\n",
    "\n",
    "#     random_result['title'] = random_novel.get('title') #corpus 구분을 위해 소설제목 추가 \n",
    "#     cor = random_novel['corpus'] \n",
    "\n",
    "#     for sentence in cor : #소설 문장 한 줄씩 \n",
    "#         input_ids = tokenizer.tokenize(sentence)\n",
    "#         sumUNK += input_ids.count('<unk>')\n",
    "#     random_result['sumUNK'] = sumUNK\n",
    "#     result.append(random_result)\n",
    "    \n",
    "# print(result)\n",
    "\n",
    "\n",
    "# 결과 : [{'title': '숫눈', 'sumUNK': 0}, {'title': '[중편연재] 누에의 난①', 'sumUNK': 2}, {'title': '건널목의 말', 'sumUNK': 2},\n",
    "#  {'title': '꽃피는 봄이 오면', 'sumUNK': 9}, {'title': '그라나다의 유기견', 'sumUNK': 1}, {'title': '잉글리시 하운드독', 'sumUNK': 1}, \n",
    "# {'title': '크리스마스에는 훌라를-장편연재 6회', 'sumUNK': 2}, {'title': '일기를 쓰는 이유', 'sumUNK': 3}, {'title': '뼈의 중량', 'sumUNK': 3}, {'title': '첫사랑', 'sumUNK': 0}]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 미완성 - {title : 소설명 , unk_list: ['<unk>': '걔', '<unk>':\"봬\", ....] , sumUNK : unk 개수}\n",
    "\n",
    "\n",
    "# result = list() \n",
    "# # model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# for random_novel in randomList:\n",
    "#     random_result = dict()\n",
    "#     sumUNK = 0 \n",
    "\n",
    "#     random_result['title'] = random_novel.get('title') #corpus 구분을 위해 소설제목 추가 \n",
    "#     cor = random_novel['corpus'] \n",
    "\n",
    "#     for sentence in cor : #소설의 한 문장씩 \n",
    "#         input_ids = tokenizer.tokenize(sentence)\n",
    "#         if input_ids.count('<unk>') != 0 : #한 문장 안에  \n",
    "#             random_result[f'corpus_sentence : {cor.index(sentence)}'] = sentence\n",
    "#             random_result[f'tokenize_result : {cor.index(sentence)}'] = input_ids\n",
    "\n",
    "#         sumUNK += input_ids.count('<unk>')\n",
    "\n",
    "#     random_result['sumUNK'] = sumUNK\n",
    "#     result.append(random_result)\n",
    "# print(result)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#training_corpus. txt 파일 만들기\n",
    "\n",
    "# with open(\"/home/jimin/workspace/voucher/whowant/training_corpus.json\" , \"r\", encoding='utf-8') as f: \n",
    "#     training_corpus= json.load(f)\n",
    "\n",
    "modified_corpus = list()\n",
    "\n",
    "# for novel in range(len(training_corpus)):\n",
    "for novel in training_corpus_list:\n",
    "    for sentence in novel.get('corpus'):\n",
    "        sentence = sentence.split(\". \")\n",
    "        join_sentence = \".\\n\".join(sentence)\n",
    "        if \"■\" in join_sentence:\n",
    "            join_sentence = join_sentence.replace(\"■\",'')\n",
    "        modified_corpus.append(join_sentence)\n",
    "    modified_corpus.append(\"\\n\")\n",
    "    modified_corpus.append(\"\\n\")\n",
    "\n",
    "print(modified_corpus)\n",
    "# with open('/home/jimin/workspace/voucher/whowant/training_corpus.txt', 'w' , encoding = 'utf-8') as f:\n",
    "#     for cor in modified_corpus:\n",
    "#             f.write(cor)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# spm 모델 학습 시키기 \n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "def makeSpmModel(vocab_size,input_file_path):\n",
    "    # parameter = '--input={} --model_prefix={} --vocab_size={}\\\n",
    "    #     --user_defined_symbols={} --model_type={} --character_coverage={}\\\n",
    "    #         --pad_id={} --unk_id={} --eos_id={} --bos_id={}'          \n",
    "\n",
    "    parameter = '--input={} --model_prefix={} --vocab_size={}\\\n",
    "         --model_type={} --character_coverage={}\\\n",
    "            '\n",
    "    # input_file = 'corpus/about_20000_corpus.txt'\n",
    "\n",
    "    vocab_size = vocab_size\n",
    "    prefix = 'novel_doc_mecab'\n",
    "    # user_defined_symbols = '(,),-,.,–,£,€,\\,[,]'\n",
    "    # control_symbols='[CLS],[SEP],[MASK]'\n",
    "    user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK]'\n",
    "    pad_id=0\n",
    "    unk_id=1\n",
    "    eos_id=-1\n",
    "    bos_id=-1\n",
    "    model_type = 'bpe'\n",
    "    character_coverage = 0.9995\n",
    "    # cmd = parameter.format(input_file, prefix, vocab_size,user_defined_symbols,model_type,\\\n",
    "    #     character_coverage,control_symbols,pad_id,unk_id,eos_id,bos_id)\n",
    "    cmd = parameter.format(input_file_path, prefix, vocab_size,model_type,\\\n",
    "        character_coverage)\n",
    "    spm.SentencePieceTrainer.Train(cmd)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "makeSpmModel('8000','training_corpus.txt')\n",
    "#10초 정도 소요 "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## koGPT2 로 UNK 사전 만들기 (0817 수정중)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\n",
    "path = \"/home/jimin/workspace/leevi-python-base\"\n",
    "if path not in sys.path: sys.path.append('/home/jimin/workspace/leevi-python-base')\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import sentencepiece as spm\n",
    "# !pip install pymongo #설치는 제대로 된 것 같은데 실행이 안됨; \n",
    "# from leevi_common.database.mongodb import MongoDB\n",
    "import random \n",
    "import json \n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>') "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "with open(\"/home/jimin/workspace/voucher/whowant/training_corpus.txt\",'r',encoding='utf-8') as corpus: \n",
    "    corpus = corpus.readlines()\n",
    "    result = list() \n",
    "    for cor in corpus:\n",
    "        result_token = tokenizer.tokenize(cor)\n",
    "        numUNK = 0 #0부터 인덱싱을 하기 위함 \n",
    "\n",
    "        if result_token.count(\"<unk>\")!=0:\n",
    "            for token in result_token: #['▁', '<unk>', '▁방으로', '▁가기', '▁위해', '선', '▁그', '▁거울', '을', '▁정면으로', '▁바라보며', '▁걸', ...] \n",
    "                if \"▁\" in token:\n",
    "                    token = token.replace(\"▁\" , '')\n",
    "\n",
    "                if token in cor: \n",
    "                    cor = cor.replace(token, '',1)\n",
    "                \n",
    "                value = cor + \"\\t\" + str(numUNK) + \"\\n\"\n",
    "            numUNK+=1 \n",
    "            print(value)\n",
    "            result.append(value)\n",
    "\n",
    "\n",
    "print(result)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#17.1\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "with open(\"/home/jimin/workspace/voucher/whowant/training_corpus.txt\",'r',encoding='utf-8') as f: \n",
    "    corpus = f.readlines()\n",
    "\n",
    "result = list() \n",
    "numUNK = 0 #0부터 인덱싱을 하기 위함 \n",
    "\n",
    "for cor in tqdm(corpus,position=0,leave=False): #한 문장씩 읽어오기 \n",
    "    result_token = tokenizer.tokenize(cor)\n",
    "\n",
    "    if \"<unk>\" not in result_token:\n",
    "        continue   \n",
    "\n",
    "    # if result_token.find(\"<unk>\") != -1: #unk 값이 있으면  \n",
    "    else:\n",
    "        tokens = [token.replace(\"▁\" , '') for token in result_token if token != '▁']\n",
    "        # preindex = None\n",
    "        # nextindex = None\n",
    "\n",
    "        # for in tokens\n",
    "\n",
    "        for token in tokens: #['▁', '<unk>', '▁방으로', '▁가기', '▁위해', '선', '▁그', '▁거울', '을', '▁정면으로', '▁바라보며', '▁걸', ...] \n",
    "            cor = cor.replace(token, '',1)\n",
    "\n",
    "            # if token in cor: \n",
    "                # cor = cor.replace(token, '',1)\n",
    "\n",
    "        # for token in result_token: #['▁', '<unk>', '▁방으로', '▁가기', '▁위해', '선', '▁그', '▁거울', '을', '▁정면으로', '▁바라보며', '▁걸', ...] \n",
    "        #     if \"▁\" in token:\n",
    "        #         token = token.replace(\"▁\" , '')\n",
    "\n",
    "        #     if token in cor: \n",
    "        #         cor = cor.replace(token, '',1)\n",
    "            \n",
    "        value = cor.strip() + \"\\t\" + str(numUNK) + \"\\n\"\n",
    "        # value = f'{cor.strip()}\\t{numUNK}\\n'\n",
    "        numUNK += 1 \n",
    "        \n",
    "        result.append(value)\n",
    "\n",
    "\n",
    "# print(result)\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jimin/workspace/voucher/whowant/training_corpus.txt'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3548249/18442939.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/jimin/workspace/voucher/whowant/training_corpus.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jimin/workspace/voucher/whowant/training_corpus.txt'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pprint.pprint(result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pprint"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "test = [re.sub(r'\\t\\d+\\n','',i) for i in result]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(set(test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(list(set(test))[10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "renew_unk = list(set([re.search('[가-힣 ]+',i).group().strip() for i in test if re.search('[가-힣 ]+',i)]))\n",
    "renew_unk.remove('')\n",
    "\n",
    "print(renew_unk)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(\"unk_original_text.txt\",'w',encoding='utf-8') as f: \n",
    "    for idx, unk in enumerate(renew_unk[1:]):\n",
    "        f.write(unk+\"\\t\"+str(idx)+\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(\"unk_original_text.txt\",'r',encoding='utf-8') as f: \n",
    "    original_text = f.readlines()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "renew_result = list()\n",
    "\n",
    "for text in original_text:\n",
    "    text = text.split(\"\\t\")[0] #문자만 가지고 오기 \n",
    "    if \" \" in text:\n",
    "        renew_text = text.split()\n",
    "        a = renew_text[0]\n",
    "        b = renew_text[len(renew_text)-1]\n",
    "\n",
    "        if a==b : \n",
    "            renew_result.append( a )\n",
    "\n",
    "        else : \n",
    "            renew_result.append(b)\n",
    "            renew_result.append(a)\n",
    "    else: \n",
    "        renew_result.append(text)     \n",
    "\n",
    "print(len(renew_result))\n",
    "renew_result = set(renew_result)\n",
    "print(len(renew_result))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(\"renew_unk_original.txt\",'w',encoding='utf-8') as f: \n",
    "    for idx, unk in enumerate(renew_result):\n",
    "        f.write(unk+\"\\t\"+str(idx)+\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}