{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !pip install transformers\n",
    "# !pip install torch\n",
    "# !pip install torch torchvision torchaudio"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "# import random \n",
    "# import json \n",
    "# import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# with open('training_corpus.json', 'r', encoding='utf-8') as f : \n",
    "#     corpus = json.load(f) #653ê°œ \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')\n",
    "# tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tokenizer.__dict__"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# randomList = random.sample(corpus, 10) #ëœë¤ìœ¼ë¡œ 10ê°œ ìƒ˜í”Œë§ \n",
    "# model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# for random_novel in randomList:\n",
    "#     cor = random_novel['corpus']\n",
    "#     for sentence in cor : \n",
    "#         input_ids = tokenizer.tokenize(sentence)\n",
    "\n",
    "#         # gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "#         #                    max_length=128,\n",
    "#         #                    repetition_penalty=2.0,\n",
    "#         #                    pad_token_id=tokenizer.pad_token_id,\n",
    "#         #                    eos_token_id=tokenizer.eos_token_id,\n",
    "#         #                    bos_token_id=tokenizer.bos_token_id,\n",
    "#         #                    use_cache=True)\n",
    "\n",
    "#         # generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "\n",
    "#         print(input_ids)\n",
    "#         # print(generated)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ë¬´ì‘ìœ„ë¡œ ì†Œì„¤ ê°€ì§€ê³  ì˜¤ê¸°"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "# from leevi_common.database.mongodb import MongoDB\n",
    "# import random \n",
    "# import json \n",
    "# import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# mongoì— ì €ì¥ë˜ì–´ìˆëŠ” ë°ì´í„° ì „ì²´ë¥¼ ê°€ì§€ê³  ì˜¤ê¸° \n",
    "\n",
    "# mongo = MongoDB(host=\"office.leevi.co.kr\", port=35005, id =\"leevi\", password = \"qlenfrl999\", database=\"whowant\" )\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')\n",
    "\n",
    "# training_corpus_list = list() #ì´ 0.8ì´ˆ ì†Œìš” / 653ê°œ \n",
    "\n",
    "# all_data = mongo.find(\"test\",{})\n",
    "\n",
    "# for x in all_data:\n",
    "#     training_corpus = dict() \n",
    "#     training_corpus['title'] = x.get('title') \n",
    "#     training_corpus['corpus'] =  x.get('corpus')\n",
    "#     training_corpus_list.append(training_corpus)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# randomList = random.sample(training_corpus_list,  10)#ëœë¤ìœ¼ë¡œ 10ê°œ ìƒ˜í”Œë§\n",
    "# randomList = random.sample(corpus, 5)#ëœë¤ìœ¼ë¡œ 1ê°œ ìƒ˜í”Œë§"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# result = list() \n",
    "# model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# for random_novel in randomList:\n",
    "#     random_result = dict()\n",
    "#     sumUNK = 0 \n",
    "#     sumToken = list()\n",
    "\n",
    "#     random_result['title'] = random_novel.get('title') #corpus êµ¬ë¶„ì„ ìœ„í•´ ì†Œì„¤ì œëª© ì¶”ê°€ \n",
    "#     cor = random_novel['corpus'] \n",
    "\n",
    "#     for sentence in cor : #ì†Œì„¤ ë¬¸ì¥ í•œ ì¤„ì”© \n",
    "#         input_ids = tokenizer.tokenize(sentence)\n",
    "#         if input_ids.count('<unk>') != 0 : \n",
    "#             random_result[f'corpus_sentence : {cor.index(sentence)}'] = sentence\n",
    "#             random_result[f'tokenize_result : {cor.index(sentence)}'] = input_ids\n",
    "\n",
    "#         sumUNK += input_ids.count('<unk>')\n",
    "\n",
    "#     random_result['sumUNK'] = sumUNK\n",
    "#     result.append(random_result)\n",
    "# print(result)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ì „ì²´ ì†Œì„¤ ê°œìˆ˜ ì¤‘ UNKì˜ ê°œìˆ˜"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# sumUNK = 0\n",
    "# for novel in cor:\n",
    "#     cor = novel['corpus'] \n",
    "\n",
    "#     for sentence in cor : #ì†Œì„¤ ë¬¸ì¥ í•œ ì¤„ì”© \n",
    "#         input_ids = tokenizer.tokenize(sentence)\n",
    "#         sumUNK += input_ids.count('<unk>')\n",
    "\n",
    "# print(sumUNK) #ì „ì²´ 653ê°œì—ì„œ ë“±ì¥í•œ UNK ê°œìˆ˜ëŠ” 1845ê°œ \n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "------------------------------------------------\r\n",
    "## 0813 UNKì˜ ê°œìˆ˜ "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# !pip install pymongo\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jimin/workspace/leevi-python-base')\n",
    "# print(sys.path)\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import sentencepiece as spm\n",
    "from leevi_common.database.mongodb import MongoDB\n",
    "import random \n",
    "import json \n",
    "import torch\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# mongoì— ì €ì¥ë˜ì–´ìˆëŠ” ë°ì´í„° ì „ì²´ë¥¼ ê°€ì§€ê³  ì˜¤ê¸° \n",
    "\n",
    "mongo = MongoDB(host=\"office.leevi.co.kr\", port=35005, id =\"leevi\", password = \"qlenfrl999\", database=\"whowant\" )\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')\n",
    "\n",
    "training_corpus_list = list() #ì´ 0.8ì´ˆ ì†Œìš” / 653ê°œ \n",
    "\n",
    "all_data = mongo.find(\"test\",{})\n",
    "\n",
    "for x in all_data:\n",
    "    training_corpus = dict() \n",
    "    training_corpus['title'] = x.get('title') \n",
    "    training_corpus['corpus'] =  x.get('corpus')\n",
    "    training_corpus_list.append(training_corpus)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# training_corpus_list = [{title:~~, corpus:~~},{title:~~, corpus:~~},...]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ì „ì²´ ë°ì´í„° ì¤‘ unkì˜ ê°œìˆ˜ \n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# sumUNK = 0\n",
    "# for novel in training_corpus_list:\n",
    "#     cor = novel['corpus'] \n",
    "\n",
    "#     for sentence in cor : #ì†Œì„¤ ë¬¸ì¥ í•œ ì¤„ì”© \n",
    "#         input_ids = tokenizer.tokenize(sentence)\n",
    "#         sumUNK += input_ids.count('<unk>')\n",
    "\n",
    "# print(sumUNK) #ì „ì²´ 653ê°œì—ì„œ ë“±ì¥í•œ UNK ê°œìˆ˜ëŠ” 1845ê°œ "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# randomList = random.sample(training_corpus_list,  10)#ëœë¤ìœ¼ë¡œ 10ê°œ ìƒ˜í”Œë§"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# {title : ì†Œì„¤ëª… , sumUNK : unk ê°œìˆ˜}\n",
    "\n",
    "# result = list() \n",
    "# # model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# for random_novel in randomList:\n",
    "#     random_result = dict()\n",
    "#     sumUNK = 0\n",
    "\n",
    "#     random_result['title'] = random_novel.get('title') #corpus êµ¬ë¶„ì„ ìœ„í•´ ì†Œì„¤ì œëª© ì¶”ê°€ \n",
    "#     cor = random_novel['corpus'] \n",
    "\n",
    "#     for sentence in cor : #ì†Œì„¤ ë¬¸ì¥ í•œ ì¤„ì”© \n",
    "#         input_ids = tokenizer.tokenize(sentence)\n",
    "#         sumUNK += input_ids.count('<unk>')\n",
    "#     random_result['sumUNK'] = sumUNK\n",
    "#     result.append(random_result)\n",
    "    \n",
    "# print(result)\n",
    "\n",
    "\n",
    "# ê²°ê³¼ : [{'title': 'ìˆ«ëˆˆ', 'sumUNK': 0}, {'title': '[ì¤‘í¸ì—°ì¬] ëˆ„ì—ì˜ ë‚œâ‘ ', 'sumUNK': 2}, {'title': 'ê±´ë„ëª©ì˜ ë§', 'sumUNK': 2},\n",
    "#  {'title': 'ê½ƒí”¼ëŠ” ë´„ì´ ì˜¤ë©´', 'sumUNK': 9}, {'title': 'ê·¸ë¼ë‚˜ë‹¤ì˜ ìœ ê¸°ê²¬', 'sumUNK': 1}, {'title': 'ì‰ê¸€ë¦¬ì‹œ í•˜ìš´ë“œë…', 'sumUNK': 1}, \n",
    "# {'title': 'í¬ë¦¬ìŠ¤ë§ˆìŠ¤ì—ëŠ” í›Œë¼ë¥¼-ì¥í¸ì—°ì¬ 6íšŒ', 'sumUNK': 2}, {'title': 'ì¼ê¸°ë¥¼ ì“°ëŠ” ì´ìœ ', 'sumUNK': 3}, {'title': 'ë¼ˆì˜ ì¤‘ëŸ‰', 'sumUNK': 3}, {'title': 'ì²«ì‚¬ë‘', 'sumUNK': 0}]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ë¯¸ì™„ì„± - {title : ì†Œì„¤ëª… , unk_list: ['<unk>': 'ê±”', '<unk>':\"ë´¬\", ....] , sumUNK : unk ê°œìˆ˜}\n",
    "\n",
    "\n",
    "# result = list() \n",
    "# # model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "# for random_novel in randomList:\n",
    "#     random_result = dict()\n",
    "#     sumUNK = 0 \n",
    "\n",
    "#     random_result['title'] = random_novel.get('title') #corpus êµ¬ë¶„ì„ ìœ„í•´ ì†Œì„¤ì œëª© ì¶”ê°€ \n",
    "#     cor = random_novel['corpus'] \n",
    "\n",
    "#     for sentence in cor : #ì†Œì„¤ì˜ í•œ ë¬¸ì¥ì”© \n",
    "#         input_ids = tokenizer.tokenize(sentence)\n",
    "#         if input_ids.count('<unk>') != 0 : #í•œ ë¬¸ì¥ ì•ˆì—  \n",
    "#             random_result[f'corpus_sentence : {cor.index(sentence)}'] = sentence\n",
    "#             random_result[f'tokenize_result : {cor.index(sentence)}'] = input_ids\n",
    "\n",
    "#         sumUNK += input_ids.count('<unk>')\n",
    "\n",
    "#     random_result['sumUNK'] = sumUNK\n",
    "#     result.append(random_result)\n",
    "# print(result)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#training_corpus. txt íŒŒì¼ ë§Œë“¤ê¸°\n",
    "\n",
    "# with open(\"/home/jimin/workspace/voucher/whowant/training_corpus.json\" , \"r\", encoding='utf-8') as f: \n",
    "#     training_corpus= json.load(f)\n",
    "\n",
    "modified_corpus = list()\n",
    "\n",
    "# for novel in range(len(training_corpus)):\n",
    "for novel in training_corpus_list:\n",
    "    for sentence in novel.get('corpus'):\n",
    "        sentence = sentence.split(\". \")\n",
    "        join_sentence = \".\\n\".join(sentence)\n",
    "        if \"â– \" in join_sentence:\n",
    "            join_sentence = join_sentence.replace(\"â– \",'')\n",
    "        modified_corpus.append(join_sentence)\n",
    "    modified_corpus.append(\"\\n\")\n",
    "    modified_corpus.append(\"\\n\")\n",
    "\n",
    "print(modified_corpus)\n",
    "# with open('/home/jimin/workspace/voucher/whowant/training_corpus.txt', 'w' , encoding = 'utf-8') as f:\n",
    "#     for cor in modified_corpus:\n",
    "#             f.write(cor)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# spm ëª¨ë¸ í•™ìŠµ ì‹œí‚¤ê¸° \n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "def makeSpmModel(vocab_size,input_file_path):\n",
    "    # parameter = '--input={} --model_prefix={} --vocab_size={}\\\n",
    "    #     --user_defined_symbols={} --model_type={} --character_coverage={}\\\n",
    "    #         --pad_id={} --unk_id={} --eos_id={} --bos_id={}'          \n",
    "\n",
    "    parameter = '--input={} --model_prefix={} --vocab_size={}\\\n",
    "         --model_type={} --character_coverage={}\\\n",
    "            '\n",
    "    # input_file = 'corpus/about_20000_corpus.txt'\n",
    "\n",
    "    vocab_size = vocab_size\n",
    "    prefix = 'novel_doc_mecab'\n",
    "    # user_defined_symbols = '(,),-,.,â€“,Â£,â‚¬,\\,[,]'\n",
    "    # control_symbols='[CLS],[SEP],[MASK]'\n",
    "    user_defined_symbols = '[PAD],[UNK],[CLS],[SEP],[MASK]'\n",
    "    pad_id=0\n",
    "    unk_id=1\n",
    "    eos_id=-1\n",
    "    bos_id=-1\n",
    "    model_type = 'bpe'\n",
    "    character_coverage = 0.9995\n",
    "    # cmd = parameter.format(input_file, prefix, vocab_size,user_defined_symbols,model_type,\\\n",
    "    #     character_coverage,control_symbols,pad_id,unk_id,eos_id,bos_id)\n",
    "    cmd = parameter.format(input_file_path, prefix, vocab_size,model_type,\\\n",
    "        character_coverage)\n",
    "    spm.SentencePieceTrainer.Train(cmd)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "makeSpmModel('8000','training_corpus.txt')\n",
    "#10ì´ˆ ì •ë„ ì†Œìš” "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## koGPT2 ë¡œ UNK ì‚¬ì „ ë§Œë“¤ê¸° (0817 ìˆ˜ì •ì¤‘)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\n",
    "path = \"/home/jimin/workspace/leevi-python-base\"\n",
    "if path not in sys.path: sys.path.append('/home/jimin/workspace/leevi-python-base')\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import sentencepiece as spm\n",
    "# !pip install pymongo #ì„¤ì¹˜ëŠ” ì œëŒ€ë¡œ ëœ ê²ƒ ê°™ì€ë° ì‹¤í–‰ì´ ì•ˆë¨; \n",
    "# from leevi_common.database.mongodb import MongoDB\n",
    "import random \n",
    "import json \n",
    "import torch"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>') "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "with open(\"/home/jimin/workspace/voucher/whowant/training_corpus.txt\",'r',encoding='utf-8') as corpus: \n",
    "    corpus = corpus.readlines()\n",
    "    result = list() \n",
    "    for cor in corpus:\n",
    "        result_token = tokenizer.tokenize(cor)\n",
    "        numUNK = 0 #0ë¶€í„° ì¸ë±ì‹±ì„ í•˜ê¸° ìœ„í•¨ \n",
    "\n",
    "        if result_token.count(\"<unk>\")!=0:\n",
    "            for token in result_token: #['â–', '<unk>', 'â–ë°©ìœ¼ë¡œ', 'â–ê°€ê¸°', 'â–ìœ„í•´', 'ì„ ', 'â–ê·¸', 'â–ê±°ìš¸', 'ì„', 'â–ì •ë©´ìœ¼ë¡œ', 'â–ë°”ë¼ë³´ë©°', 'â–ê±¸', ...] \n",
    "                if \"â–\" in token:\n",
    "                    token = token.replace(\"â–\" , '')\n",
    "\n",
    "                if token in cor: \n",
    "                    cor = cor.replace(token, '',1)\n",
    "                \n",
    "                value = cor + \"\\t\" + str(numUNK) + \"\\n\"\n",
    "            numUNK+=1 \n",
    "            print(value)\n",
    "            result.append(value)\n",
    "\n",
    "\n",
    "print(result)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#17.1\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "with open(\"/home/jimin/workspace/voucher/whowant/training_corpus.txt\",'r',encoding='utf-8') as f: \n",
    "    corpus = f.readlines()\n",
    "\n",
    "result = list() \n",
    "numUNK = 0 #0ë¶€í„° ì¸ë±ì‹±ì„ í•˜ê¸° ìœ„í•¨ \n",
    "\n",
    "for cor in tqdm(corpus,position=0,leave=False): #í•œ ë¬¸ì¥ì”© ì½ì–´ì˜¤ê¸° \n",
    "    result_token = tokenizer.tokenize(cor)\n",
    "\n",
    "    if \"<unk>\" not in result_token:\n",
    "        continue   \n",
    "\n",
    "    # if result_token.find(\"<unk>\") != -1: #unk ê°’ì´ ìˆìœ¼ë©´  \n",
    "    else:\n",
    "        tokens = [token.replace(\"â–\" , '') for token in result_token if token != 'â–']\n",
    "        # preindex = None\n",
    "        # nextindex = None\n",
    "\n",
    "        # for in tokens\n",
    "\n",
    "        for token in tokens: #['â–', '<unk>', 'â–ë°©ìœ¼ë¡œ', 'â–ê°€ê¸°', 'â–ìœ„í•´', 'ì„ ', 'â–ê·¸', 'â–ê±°ìš¸', 'ì„', 'â–ì •ë©´ìœ¼ë¡œ', 'â–ë°”ë¼ë³´ë©°', 'â–ê±¸', ...] \n",
    "            cor = cor.replace(token, '',1)\n",
    "\n",
    "            # if token in cor: \n",
    "                # cor = cor.replace(token, '',1)\n",
    "\n",
    "        # for token in result_token: #['â–', '<unk>', 'â–ë°©ìœ¼ë¡œ', 'â–ê°€ê¸°', 'â–ìœ„í•´', 'ì„ ', 'â–ê·¸', 'â–ê±°ìš¸', 'ì„', 'â–ì •ë©´ìœ¼ë¡œ', 'â–ë°”ë¼ë³´ë©°', 'â–ê±¸', ...] \n",
    "        #     if \"â–\" in token:\n",
    "        #         token = token.replace(\"â–\" , '')\n",
    "\n",
    "        #     if token in cor: \n",
    "        #         cor = cor.replace(token, '',1)\n",
    "            \n",
    "        value = cor.strip() + \"\\t\" + str(numUNK) + \"\\n\"\n",
    "        # value = f'{cor.strip()}\\t{numUNK}\\n'\n",
    "        numUNK += 1 \n",
    "        \n",
    "        result.append(value)\n",
    "\n",
    "\n",
    "# print(result)\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jimin/workspace/voucher/whowant/training_corpus.txt'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3548249/18442939.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/jimin/workspace/voucher/whowant/training_corpus.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jimin/workspace/voucher/whowant/training_corpus.txt'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pprint.pprint(result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pprint"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "\n",
    "test = [re.sub(r'\\t\\d+\\n','',i) for i in result]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(set(test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(list(set(test))[10])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "renew_unk = list(set([re.search('[ê°€-í£ ]+',i).group().strip() for i in test if re.search('[ê°€-í£ ]+',i)]))\n",
    "renew_unk.remove('')\n",
    "\n",
    "print(renew_unk)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(\"unk_original_text.txt\",'w',encoding='utf-8') as f: \n",
    "    for idx, unk in enumerate(renew_unk[1:]):\n",
    "        f.write(unk+\"\\t\"+str(idx)+\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(\"unk_original_text.txt\",'r',encoding='utf-8') as f: \n",
    "    original_text = f.readlines()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "renew_result = list()\n",
    "\n",
    "for text in original_text:\n",
    "    text = text.split(\"\\t\")[0] #ë¬¸ìë§Œ ê°€ì§€ê³  ì˜¤ê¸° \n",
    "    if \" \" in text:\n",
    "        renew_text = text.split()\n",
    "        a = renew_text[0]\n",
    "        b = renew_text[len(renew_text)-1]\n",
    "\n",
    "        if a==b : \n",
    "            renew_result.append( a )\n",
    "\n",
    "        else : \n",
    "            renew_result.append(b)\n",
    "            renew_result.append(a)\n",
    "    else: \n",
    "        renew_result.append(text)     \n",
    "\n",
    "print(len(renew_result))\n",
    "renew_result = set(renew_result)\n",
    "print(len(renew_result))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(\"renew_unk_original.txt\",'w',encoding='utf-8') as f: \n",
    "    for idx, unk in enumerate(renew_result):\n",
    "        f.write(unk+\"\\t\"+str(idx)+\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}